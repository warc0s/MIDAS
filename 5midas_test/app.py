import streamlit as st
import os
from agents_test import process_joblib, generate_markdown_report, group_manager, groupchat

st.title("MIDAS TEST: An√°lisis de Modelos de Machine Learning")

uploaded_file = st.file_uploader("üìÇ Carga un archivo .joblib", type=["joblib"])

if uploaded_file:
    file_path = "temp_model.joblib"
    with open(file_path, "wb") as f:
        f.write(uploaded_file.getbuffer())

    st.subheader("‚öôÔ∏è Procesando el modelo...")
    model_info = process_joblib(file_path)

    if "error" in model_info:
        st.error(model_info["error"])
    else:
        st.success("‚úÖ Modelo analizado con √©xito")
        # Bot√≥n para iniciar la evaluaci√≥n con los agentes
        if st.button("üîÑ Iniciar Evaluaci√≥n con los Agentes"):
            group_manager.initiate_chat(
                recipient=group_manager,
                max_turns=3,  # Puedes ajustar este valor
                message=f"Analyze the following ML model: {model_info} and generate a Markdown report."
            )
            st.success("‚úÖ Evaluaci√≥n en proceso. Espera unos segundos...")

        # Bot√≥n para finalizar la evaluaci√≥n y descargar el reporte
        if st.button("üìÑ Finalizar An√°lisis y Descargar Reporte"):
            if groupchat.messages:

                st.subheader("üìä Resultados del Modelo")
                with st.expander("üìå Informaci√≥n del Modelo"):
                    st.write(f"**Tiempo de carga:** {model_info['load_time']:.4f} segundos")
                    st.write(f"**Tama√±o en disco:** {model_info['size_on_disk']:.4f} MB")
                    st.write(f"**Uso de memoria:** {model_info['memory_usage']:.2f} MB")
                    st.write(f"**Rendimiento:** {model_info['throughput']:.2f} muestras/segundo")
                    st.write(f"**Recomendaci√≥n final:** {model_info['final_recommendation']}")
                
                with st.expander("üìà M√©tricas de Rendimiento"):
                    perf = model_info['performance_metrics']
                    st.write(f"- **Pico de memoria:** {perf['memory_peak']}")
                    st.write(f"- **Uso de CPU:** {perf['cpu_usage']}%")
                    st.write(f"- **Tiempo de predicci√≥n:** {perf['prediction_time']:.4f} segundos")
                
                with st.expander("‚ö†Ô∏è Pruebas de Robustez"):
                    robustness = model_info['robustness_tests']
                    st.write(f"- **Valores nulos:** {robustness['null_values']}")
                    st.write(f"- **Fuera de rango:** {robustness['out_of_range']}")
                    st.write(f"- **Tipo de datos incorrecto:** {robustness['wrong_data_type']}")
                    st.write(f"- **Predicciones consistentes:** {robustness['consistent_predictions']}")

                st.subheader("üìÑ Reporte Generado")
                report = generate_markdown_report(groupchat.messages)
                with open("informe_analisis_modelo.md", "r", encoding="utf-8") as f:
                    st.download_button("‚¨áÔ∏è Descargar Reporte", f, "informe_analisis_modelo.md", "text/markdown")
            else:
                st.warning("‚ö†Ô∏è No hay mensajes en el chat de grupo. No se gener√≥ reporte.")

    os.remove(file_path)  # Limpieza del archivo temporal
