# Midas Test

## Descripci√≥n General

MIDAS Test es el componente MIDAS especializado en la evaluaci√≥n exhaustiva de modelos de machine learning almacenados en formato joblib. Su prop√≥sito principal es analizar la calidad, rendimiento y robustez de modelos ML mediante una arquitectura de agentes conversacionales basados en IA.

El sistema utiliza Large Language Models (LLM) para coordinar m√∫ltiples agentes especializados que eval√∫an diferentes aspectos de los modelos ML. MIDAS Test implementa un enfoque de colaboraci√≥n multi-agente donde cada agente aporta su perspectiva especializada para generar un informe completo.

MIDAS Test se basa en el framework AG2 para la gesti√≥n de agentes conversacionales y utiliza Streamlit para proporcionar una interfaz de usuario accesible.

## Arquitectura T√©cnica

### Backend:

- **Lenguaje y Bibliotecas:** 
  - Python 3.x
  - AG2 para la gesti√≥n de agentes IA
  - Scikit-learn para manipulaci√≥n de modelos ML
  - Joblib para carga/guardado de modelos
  - DeepInfra API para acceder a modelos LLM
  - deep_translator para traducir informes al espa√±ol

- **Componentes Clave:**
  - *Agentes Especializados*:
    - **Model Analyzer**: Examina la estructura y caracter√≠sticas del modelo ML.
    - **Performance Tester**: Eval√∫a m√©tricas de rendimiento como latencia, uso de memoria y CPU.
    - **Robustness Checker**: Verifica la resistencia del modelo ante entradas an√≥malas.
    - **Output Validator**: Confirma la validez y formato de las predicciones del modelo.
  
  - *Gestor de Comunicaci√≥n*:
    - **GroupChat**: Facilita la comunicaci√≥n entre agentes.
    - **GroupChatManager**: Coordina el flujo de la conversaci√≥n y turno de los agentes.
  
  - *Modelo LLM Base*:
    - Utiliza *meta-llama/Llama-3.3-70B-Instruct-Turbo* a trav√©s de la API de DeepInfra.
    - Configuraci√≥n personalizable de temperatura y seed para resultados consistentes.
  
  - *M√≥dulos de Procesamiento*:
    - **load_model**: Carga modelos joblib y mide tiempo de carga.
    - **check_model_validity**: Verifica si el modelo es compatible con Scikit-learn.
    - **measure_latency**: Eval√∫a tiempos de respuesta en diferentes tama√±os de batch.
    - **measure_memory_usage**: Mide el uso de memoria.
    - **measure_memory_and_cpu_during_prediction**: Eval√∫a el uso de recursos durante predicciones.
    - **validate_predictions**: Verifica la consistencia y formato de las predicciones.
    - **check_robustness**: Prueba comportamiento ante valores nulos, extremos y tipos incorrectos.
    - **translate_to_spanish**: Traduce el informe al espa√±ol.
    - **generate_markdown_report**: Compila los hallazgos en formato Markdown estructurado.

- **Flujo de Procesamiento**:
  1. Carga del modelo joblib.
  2. Validaci√≥n inicial del modelo (compatibilidad con Scikit-learn).
  3. Generaci√≥n de datos de muestra para pruebas.
  4. Ejecuci√≥n de pruebas de rendimiento, robustez y validaci√≥n.
  5. Compilaci√≥n de m√©tricas y resultados.
  6. Activaci√≥n de agentes IA para an√°lisis especializado.
  7. Generaci√≥n de informe final en formato Markdown en espa√±ol.

### Frontend:

- **Tecnolog√≠as:**
  - Streamlit para la interfaz web interactiva
  - Componentes UI de Streamlit: file_uploader, expanders, download_button

- **Estructura de la Interfaz:**
  - Secci√≥n de carga de archivos
  - Panel de progreso y estado
  - Visualizaci√≥n de resultados en secciones expandibles
  - Botones para iniciar evaluaci√≥n y descargar informes

## Funcionalidad

- **An√°lisis de Modelos ML**: Eval√∫a m√∫ltiples aspectos del modelo incluyendo validez, rendimiento y robustez.

- **M√©tricas de Rendimiento**: 
  - Tiempo de carga del modelo
  - Uso de memoria durante predicciones
  - Utilizaci√≥n de CPU
  - Latencia en diferentes tama√±os de batch (1, 100, 1000, 10000)
  - Throughput (predicciones por segundo)

- **Pruebas de Robustez**:
  - Comportamiento ante valores nulos
  - Resistencia a valores fuera de rango
  - Manejo de tipos de datos incorrectos
  - Consistencia de predicciones

- **Validaci√≥n de Salidas**:
  - Verificaci√≥n de formato correcto (array NumPy)
  - Validaci√≥n de rangos de valores
  - Comprobaci√≥n de suma de probabilidades igual a 1 (cuando aplica)

- **Recomendaci√≥n Automatizada**: Clasificaci√≥n del modelo como "APTO" o "NO APTO" basada en la validez del modelo y la consistencia de sus predicciones.

- **Reporte Markdown**: Generaci√≥n autom√°tica de documentaci√≥n estructurada en espa√±ol con los hallazgos y recomendaciones.

## Gu√≠a de Uso

### A trav√©s de la Interfaz Web (Streamlit):

1. Inicie la aplicaci√≥n ejecutando:
   *streamlit run app.py*

2. En la interfaz web, haga clic en el cargador de archivos y seleccione el modelo joblib a evaluar.

3. Una vez cargado el modelo, pulse el bot√≥n "üîÑ Iniciar Evaluaci√≥n con los Agentes" para comenzar el an√°lisis.

4. El sistema mostrar√° un mensaje indicando que la evaluaci√≥n est√° en proceso.

5. Despu√©s de unos 90 segundos, pulse "üìÑ Finalizar An√°lisis y Descargar Reporte" para ver y descargar los resultados.

6. Explore los resultados en las secciones expandibles:
   - "üìå Informaci√≥n del Modelo": Datos b√°sicos como tiempo de carga y tama√±o
   - "üìà M√©tricas de Rendimiento": Detalles sobre uso de recursos
   - "‚ö†Ô∏è Pruebas de Robustez": Resultados de las pruebas de resistencia

7. Descargue el informe completo en formato Markdown utilizando el bot√≥n "‚¨áÔ∏è Descargar Reporte".

### Mediante L√≠nea de Comandos:

1. Ejecute el script principal:
   *python agents_test.py*

2. Cuando se solicite, ingrese la ruta completa al archivo joblib que desea analizar.

3. El sistema ejecutar√° autom√°ticamente todas las pruebas y generar√° un informe en el archivo "informe_analisis_modelo.md".

### Ejemplo de Salida:

El reporte generado contendr√° secciones como:

# üìä Informe de An√°lisis del Modelo
**Generado el:** 2025-03-02 15:30:45

---

## üîç Resumen del Modelo
[Informaci√≥n general sobre el modelo y sus caracter√≠sticas]

## ‚öôÔ∏è M√©tricas de Rendimiento
[Detalles sobre rendimiento, memoria y CPU]

## ‚è≥ An√°lisis de Latencia
[An√°lisis de tiempos de respuesta]

## ‚úÖ Validez de Predicciones
[Validaci√≥n de las salidas del modelo]

## üõ°Ô∏è Pruebas de Robustez
[Resultados de pruebas de resistencia]

## üìå Recomendaci√≥n Final
**APTO**

## üîß Sugerencias de Mejora
[Recomendaciones para mejorar el modelo]

## Limitaciones Actuales

- El componente est√° optimizado para modelos Scikit-learn y puede tener limitaciones con otros frameworks de ML.
- Las pruebas de robustez son b√°sicas y no cubren todos los escenarios posibles de entrada an√≥mala.
- La evaluaci√≥n actual se centra en la validez del modelo y consistencia de predicciones, sin m√©tricas espec√≠ficas de calidad predictiva.
- El rendimiento de los agentes puede variar dependiendo de la calidad de las respuestas del LLM utilizado.
- La traducci√≥n autom√°tica al espa√±ol puede contener imprecisiones en terminolog√≠a t√©cnica.